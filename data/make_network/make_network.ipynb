{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "from detoxify import Detoxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Melissa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Melissa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Melissa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon') # get lexicons data\n",
    "nltk.download('punkt') # for tokenizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(df):\n",
    "    # Time range of posts:\n",
    "    times = [int(x) for x in df['created_utc'].tolist()]\n",
    "    bins = range(min(times)-1, max(times)+1, 86400)\n",
    "    counts, bins = np.histogram(times, bins)\n",
    "    plt.figure()\n",
    "    plt.title(f\"UTC chunked by day from {datetime.fromtimestamp(min(times))} to {datetime.fromtimestamp(max(times))}\")\n",
    "    plt.stairs(counts, bins)\n",
    "    print(f\"Ranking most common subreddit: {df['subreddit'].value_counts()}\")\n",
    "    print(f\"Ranking most common user: {df['author'].value_counts()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(df):\n",
    "    print(f\"Original df size: {df.shape}\")\n",
    "    # Remove rows that don't make sense (eg. UTC not an integer)\n",
    "    df['created_utc'] = pd.to_numeric(df['created_utc'], errors='coerce')\n",
    "    df = df.dropna(subset=['created_utc'])\n",
    "    print(f\"After removing messed up rows: {df.shape}\")\n",
    "    # Removes rows that are too old (before 2020)\n",
    "    # df['created_utc'] = df['created_utc'].astype('int')\n",
    "    # df = df[df['created_utc'] > datetime(2020, 1, 1).timestamp()]\n",
    "    # Assign post ID's to link_id, remove the starting 't3_'\n",
    "    df.loc[df['type'] == 'post', 'link_id'] = df.loc[df['type'] == 'post', 'id']\n",
    "    df['link_id'] = df['link_id'].str.replace('t3_', '')\n",
    "    df['parent_id'] = df['parent_id'].str.replace('t3_', '')\n",
    "    # Removing known bots\n",
    "    authors_to_remove = [\"AutoModerator\", \"#NAME?\"]\n",
    "    df = df[~df['author'].isin(authors_to_remove)]\n",
    "    print(f\"After removing known bots df size: {df.shape}\")\n",
    "    # Shrink set to desired features. Drop na author/subreddit/id/link_id\n",
    "    df['num_comments'].fillna(0, inplace=True)\n",
    "    df = df[['created_utc', 'type', 'author', 'subreddit', 'id', 'link_id', 'score', 'num_comments', 'upvote_ratio', 'neg', 'neu', 'pos', 'compound']].dropna(subset=['author', 'subreddit', 'id', 'link_id'])\n",
    "    df = df.drop_duplicates(subset=[\"author\", \"id\"]).reset_index(drop=True)\n",
    "    print(f\"After drop dup author/link id and NaN df size: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_min_commenters(df, x=10):\n",
    "    # Drop duplicate ID's: each post/comment should have unique id:\n",
    "    df = df.drop_duplicates(subset=['id'])\n",
    "    # Filter out min commenters\n",
    "    print(f\"Before filtering out authors less than {x} comments: {df.shape}\")\n",
    "    filter_comment_authors = df[\"type\"] != \"comment\"\n",
    "    value_counts = df['author'].value_counts()\n",
    "    mask = df['author'].map(value_counts) > x\n",
    "    df = df[mask | filter_comment_authors]\n",
    "    print(f\"After filtering out authors less than {x} comments: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    counter = Counter(lst)\n",
    "    return counter.most_common(1)[0][0] if counter else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter None and empty strings from a list\n",
    "def filter_list(lst):\n",
    "    return ' '.join([item for item in lst if (item is not None) and (item != '') and not (isinstance(item, float) and math.isnan(item))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sentiment_analysis(df):\n",
    "    sia = SIA()\n",
    "    cols = ['title', 'selftext', 'body']\n",
    "    df[cols] = df[cols].fillna('')\n",
    "    print(\"Concat content..\")\n",
    "    df[\"selftext_title_body\"] = df[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "    print(\"Obtaining polarity scores\")\n",
    "    res = [*df['selftext_title_body'].apply(sia.polarity_scores)]\n",
    "    print(\"Creating new dataframe...\")\n",
    "    sentiment_df = pd.DataFrame.from_records(res)\n",
    "    df = pd.concat([df, sentiment_df], axis=1, join='inner')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_toxic_analysis(df):\n",
    "    pred = Detoxify('multilingual')\n",
    "    # pred = Detoxify('unbiased')\n",
    "    cols = ['title', 'selftext', 'body']\n",
    "    df[cols] = df[cols].fillna('')\n",
    "    print(\"Concat content..\")\n",
    "    df[\"selftext_title_body\"] = df[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "    users = pd.read_csv(\"all_users_check_susp.csv\", index_col=0)\n",
    "    filt_df = df[df['author'].isin(users['User'])].reset_index(drop=True)\n",
    "    print(f\"Obtaining toxicity scores on size {filt_df.shape}\")\n",
    "    res = [*filt_df['selftext_title_body'].apply(pred.predict)]\n",
    "    print(\"Creating new dataframe...\")\n",
    "    sentiment_df = pd.DataFrame.from_records(res).reset_index(drop=True)\n",
    "    filt_df = pd.concat([filt_df, sentiment_df], axis=1, join='inner').reset_index(drop=True)\n",
    "    df = pd.merge(df, filt_df[['id', 'toxicity']], on='id', how='left').drop_duplicates()\n",
    "    return df#, sentiment_df, filt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating similarity on size (2855, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>jw_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Quothe-</td>\n",
       "      <td>0.508074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0U8124X</td>\n",
       "      <td>0.538679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120GoHogs120</td>\n",
       "      <td>0.592851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1984rip</td>\n",
       "      <td>0.597691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1bir</td>\n",
       "      <td>0.543934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>zihuatapulco</td>\n",
       "      <td>0.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>zombi-roboto</td>\n",
       "      <td>0.603211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>zoot_boy</td>\n",
       "      <td>0.521284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>zubaz69</td>\n",
       "      <td>0.574121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>zviwkls</td>\n",
       "      <td>0.580124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1139 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            author    jw_sim\n",
       "0         -Quothe-  0.508074\n",
       "1          0U8124X  0.538679\n",
       "2     120GoHogs120  0.592851\n",
       "3          1984rip  0.597691\n",
       "4             1bir  0.543934\n",
       "...            ...       ...\n",
       "1134  zihuatapulco  0.595900\n",
       "1135  zombi-roboto  0.603211\n",
       "1136      zoot_boy  0.521284\n",
       "1137       zubaz69  0.574121\n",
       "1138       zviwkls  0.580124\n",
       "\n",
       "[1139 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "# print(Levenshtein.jaro_winkler(\"This is the best video ever\", \"It's the best video\"))\n",
    "def dist(x, all_comments):\n",
    "    return mean([Levenshtein.jaro_winkler(x, y) for y in all_comments])\n",
    "    \n",
    "\n",
    "def find_comment_similarity(df):\n",
    "    users = pd.read_csv(\"all_users_check_susp_parrot.csv\", index_col=0)\n",
    "    df_comments = df[df['type'] == 'comment'][['author', 'body']].reset_index(drop=True)\n",
    "    df_comments = df_comments[df_comments['author'] != 'AutoModerator']\n",
    "    all_comments = df_comments['body'].tolist()\n",
    "    df_comments_rel_users = df_comments[df_comments['author'].isin(users['User'])].reset_index(drop=True)\n",
    "    print(f\"Calculating similarity on size {df_comments_rel_users.shape}\")\n",
    "    df_comments_rel_users['jw_sim'] = df_comments_rel_users['body'].apply(lambda x: dist(x, all_comments))\n",
    "    return df_comments_rel_users[['author', 'jw_sim']].groupby(['author']).mean().reset_index()\n",
    "\n",
    "df = find_comment_similarity(pd.read_csv(os.path.join(\"data_scraping\", \"political_subreddits_new.csv\"))[:10000])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get_subreddit_info.ipynb',\n",
       " 'political_center.csv',\n",
       " 'political_communism.csv',\n",
       " 'political_left.csv',\n",
       " 'political_libertarian.csv',\n",
       " 'political_other.csv',\n",
       " 'political_right.csv',\n",
       " 'popular_by_post_votes.csv',\n",
       " 'popular_by_subscribers.csv',\n",
       " 'save',\n",
       " 'sus_usrs_subreddits.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_dir = os.path.join('data_scraping', 'subreddits')\n",
    "os.listdir(sr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create usr_df, sr_df that has corresponding node attributes\n",
    "# - usr_df['usr', 'top_sr', 'sr_set', 'num_sr', 'num_posts', 'num_comments', 'num_popular_sr', 'num_political_sr', 'num_misc_sr', 'ratio_popular_sr', 'ratio_political_sr', 'ratio_misc_sr', 'avg_upvote_ratio', 'content_list']\n",
    "# - sr_df = the csv scraped\n",
    "def get_node_attr(df, no_set=True):\n",
    "    ### Put together sr_df\n",
    "    print(\"=== Making SR DF ===\")\n",
    "    sr_dir = os.path.join('data_scraping', 'subreddits')\n",
    "    political_df_list = []\n",
    "    popular_df_list = []\n",
    "    misc_df_list = []\n",
    "    for file in os.listdir(sr_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            print(f\"Parsing {file}\")\n",
    "            cur_df = pd.read_csv(os.path.join(sr_dir, file))\n",
    "            if 'political' in file:\n",
    "                political_df_list.append(cur_df)\n",
    "            elif 'popular' in file:\n",
    "                popular_df_list.append(cur_df)\n",
    "            else:\n",
    "                misc_df_list.append(cur_df)\n",
    "    political_df = pd.concat(political_df_list)\n",
    "    political_df['topic_type'] = 'political'\n",
    "    popular_df = pd.concat(popular_df_list)\n",
    "    popular_df['topic_type'] = 'popular'\n",
    "    misc_df = pd.concat(misc_df_list)\n",
    "    misc_df['topic_type'] = 'misc'\n",
    "    sr_df = pd.concat([political_df, popular_df, misc_df]).drop_duplicates(subset='Subreddit').reset_index(drop=True)[['Subreddit', 'Title', 'Public Description', 'Description', 'topic_type']]\n",
    "    sr_df['node_type'] = 'subreddit'\n",
    "    \n",
    "    ### Put together usr_df\n",
    "    print(\"=== Setting up for USR DF ===\")\n",
    "    df = df[['type', 'author', 'subreddit', 'upvote_ratio', 'neg', 'neu', 'pos', 'compound']]\n",
    "    df = df.groupby(['author']).agg(list).reset_index()\n",
    "    df['subreddit_set'] = df['subreddit'].apply(lambda x: set(x))\n",
    "    df['popular_sr'] = [set(sr_df[sr_df['topic_type'] == 'popular']['Subreddit']).intersection(x) for x in df['subreddit_set']]\n",
    "    df['political_sr'] = [set(sr_df[sr_df['topic_type'] == 'political']['Subreddit']).intersection(x) for x in df['subreddit_set']]\n",
    "    df['misc_sr'] = [set(sr_df[sr_df['topic_type'] == 'misc']['Subreddit']).intersection(x) for x in df['subreddit_set']]\n",
    "    print(\"=== Making USR DF ===\")\n",
    "    usr_df = df[['author']].rename(columns={'author': 'usr'}).drop_duplicates(subset='usr').reset_index(drop=True)\n",
    "    usr_df['top_sr'] = df['subreddit'].apply(most_common)\n",
    "    if no_set:\n",
    "        usr_df['sr_set'] = df['subreddit_set'].apply(lambda x: ' '.join(x))\n",
    "    else:\n",
    "        usr_df['sr_set'] = df['subreddit_set']\n",
    "    print(\"Counting...\")\n",
    "    usr_df['num_sr'] = df['subreddit_set'].apply(lambda x: len(x))\n",
    "    usr_df['num_posts'] = df['type'].apply(lambda x: x.count('post'))\n",
    "    usr_df['num_comments'] = df['type'].apply(lambda x: x.count('comment'))\n",
    "    usr_df['num_popular_sr'] = df['popular_sr'].apply(lambda x: len(x))\n",
    "    usr_df['num_political_sr'] = df['political_sr'].apply(lambda x: len(x))\n",
    "    usr_df['num_misc_sr'] = df['misc_sr'].apply(lambda x: len(x))\n",
    "    print(\"Adding ratios...\")\n",
    "    usr_df['ratio_popular_sr'] = usr_df['num_popular_sr'] / usr_df['num_sr']\n",
    "    usr_df['ratio_political_sr'] = usr_df['num_political_sr'] / usr_df['num_sr']\n",
    "    usr_df['ratio_misc_sr'] = usr_df['num_misc_sr'] / usr_df['num_sr']\n",
    "    usr_df['avg_upvote_ratio'] = df['upvote_ratio'].apply(lambda x: np.nanmean(x))\n",
    "    print(\"Adding sentiment analysis...\")\n",
    "    usr_df['avg_neg'] = df['neg'].apply(lambda x: np.nanmean(x))\n",
    "    usr_df['avg_neu'] = df['neu'].apply(lambda x: np.nanmean(x))\n",
    "    usr_df['avg_pos'] = df['pos'].apply(lambda x: np.nanmean(x))\n",
    "    usr_df['avg_compound'] = df['compound'].apply(lambda x: np.nanmean(x))\n",
    "    # usr_df['content_dump'] = df['selftext'] + df['title'] + df['body']\n",
    "    # if no_set:\n",
    "    #     usr_df['content_dump'] = usr_df['content_dump'].apply(filter_list)\n",
    "    usr_df['node_type'] = 'user'\n",
    "\n",
    "    return sr_df, usr_df, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sus_usr(df):\n",
    "    sus_usrs_df = pd.read_csv(\"sus_users.csv\", index_col=False)\n",
    "    df[\"is_sus\"] = df['usr'].isin(sus_usrs_df['name'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes: subreddits, users\n",
    "# edges: posters connected to subreddits, commenters connected to posters\n",
    "# return: df['node_x'(subreddit/poster), 'node_y'(poster/commenter), edge_attr='weight']\n",
    "def make_bipartite_graph(df):\n",
    "    ### First, connect subreddits to posters\n",
    "    df_1 = df[df['type'] == 'post']\n",
    "    df_1 = df_1[['subreddit', 'author']]\n",
    "    df_1['weight'] = df_1.groupby(['subreddit', 'author'])['subreddit'].transform('count')\n",
    "    df_1 = df_1.drop_duplicates(subset=['subreddit', 'author']).reset_index(drop=True)\n",
    "    df_1.rename(columns={'subreddit': 'node_x', 'author': 'node_y'}, inplace=True)\n",
    "\n",
    "    ### Second, connect posters to commenters:\n",
    "    df = df[['type', 'subreddit', 'author', 'id', 'link_id']].drop_duplicates(subset=['id']) # make sure we don't have multiple of same ID (each comment/post should have unique)\n",
    "    df = df.merge(df,  left_on='id', right_on='link_id', how='inner')\n",
    "    df['id'] = df['id_x']\n",
    "    df['subreddit'] = df['subreddit_x']\n",
    "    df = df[['author_x', 'id', 'subreddit', 'author_y']]\n",
    "    df = df.drop_duplicates(subset=[\"author_x\", \"author_y\", 'id']).reset_index()\n",
    "    # No usr to self pairing\n",
    "    df = df[df['author_x'] != df['author_y']]\n",
    "    # Sort pairs so we can squash\n",
    "    df.loc[df['author_x'] > df['author_y'], ['author_x', 'author_y']] = df.loc[df['author_x'] > df['author_y'], ['author_y', 'author_x']]\n",
    "    # Squash all same usr-usr pairings link_ids into list\n",
    "    df = df.groupby(['author_x', 'author_y']).agg(list).reset_index()\n",
    "    # df.to_csv(\"check0.csv\")\n",
    "    df['weight'] = df['id'].str.len()\n",
    "    # Do we want?\n",
    "    # df['top_subreddit'] = df['subreddit'].apply(most_common)\n",
    "    # Sort by weight, only keep necessary columns\n",
    "    df = df[['author_x', 'author_y', 'weight']] #, 'top_subreddit']]\n",
    "    df.rename(columns={'author_x': 'node_x', 'author_y': 'node_y'}, inplace=True)\n",
    "\n",
    "    print(f\"Size of subreddit-poster: {df_1.shape}\")\n",
    "    print(f\"Size of poster-commenter: {df.shape}\")\n",
    "    return pd.concat([df_1, df]).sort_values(by='weight', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node_attr(sr_df, usr_df, G):\n",
    "    attrs = ['node_type',\n",
    "    'usr',\n",
    "    'top_sr',\n",
    "    'sr_set',\n",
    "    'num_sr',\n",
    "    'num_posts',\n",
    "    'num_comments',\n",
    "    'num_popular_sr',\n",
    "    'num_political_sr',\n",
    "    'num_misc_sr',\n",
    "    'ratio_popular_sr',\n",
    "    'ratio_political_sr',\n",
    "    'ratio_misc_sr',\n",
    "    'avg_upvote_ratio',\n",
    "    'avg_neg',\n",
    "    'avg_neu',\n",
    "    'avg_pos',\n",
    "    'avg_compound',\n",
    "    'is_sus',\n",
    "    'Subreddit',\n",
    "    'Title',\n",
    "    'Public Description',\n",
    "    'Description',\n",
    "    'topic_type']\n",
    "    attrs_dict =  dict.fromkeys(attrs, 0)\n",
    "    attrs_dict['node_type'] = 1\n",
    "    print(\"Setting subreddit attributes...\")\n",
    "    sr_df = sr_df.set_index('Subreddit')\n",
    "    nx.set_node_attributes(G, sr_df.to_dict('index'))\n",
    "    print(\"Setting user attributes...\")\n",
    "    usr_df = usr_df.set_index('usr')\n",
    "    nx.set_node_attributes(G, usr_df.to_dict('index'))\n",
    "    print(\"Setting default 0 values for nodes that didn't have attributes, marking node_type as subreddit...\")\n",
    "    for node in G.nodes(data=True):\n",
    "        if len(node[1]) == 0:\n",
    "            G.add_nodes_from([(node[0], attrs_dict)])\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Network(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data_scraping\\\\data_1month\" # Change this to path where .csv are located\n",
    "out_path = \"output\"\n",
    "mega_df = pd.DataFrame()\n",
    "combo_category_df = {\n",
    "    \"political\": pd.DataFrame(),\n",
    "    \"popular\": pd.DataFrame(),\n",
    "    \"sus_usr\": pd.DataFrame(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update mega attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parsing political_subreddits_new.csv (282927, 22) ===\n",
      "Original df size: (282927, 22)\n",
      "After removing messed up rows: (282927, 22)\n",
      "After removing known bots df size: (278127, 22)\n",
      "After drop dup author/link id and NaN df size: (269620, 13)\n",
      "=== Parsing popular_subreddits_new.csv (375710, 22) ===\n",
      "Original df size: (375710, 22)\n",
      "After removing messed up rows: (375710, 22)\n",
      "After removing known bots df size: (374114, 22)\n",
      "After drop dup author/link id and NaN df size: (366073, 13)\n",
      "=== Parsing sus_usr_subreddits_0_new.csv (687008, 22) ===\n",
      "Original df size: (687008, 22)\n",
      "After removing messed up rows: (687008, 22)\n",
      "After removing known bots df size: (679397, 22)\n",
      "After drop dup author/link id and NaN df size: (664478, 13)\n",
      "=== Parsing sus_usr_subreddits_10_new.csv (49622, 22) ===\n",
      "Original df size: (49622, 22)\n",
      "After removing messed up rows: (49622, 22)\n",
      "After removing known bots df size: (48988, 22)\n",
      "After drop dup author/link id and NaN df size: (48150, 13)\n",
      "=== Parsing sus_usr_subreddits_1_new.csv (667336, 22) ===\n",
      "Original df size: (667336, 22)\n",
      "After removing messed up rows: (662925, 22)\n",
      "After removing known bots df size: (656490, 22)\n",
      "After drop dup author/link id and NaN df size: (644026, 13)\n",
      "=== Parsing sus_usr_subreddits_2_new.csv (238142, 22) ===\n",
      "Original df size: (238142, 22)\n",
      "After removing messed up rows: (238142, 22)\n",
      "After removing known bots df size: (235820, 22)\n",
      "After drop dup author/link id and NaN df size: (230597, 13)\n",
      "=== Parsing sus_usr_subreddits_3_new.csv (995680, 22) ===\n",
      "Original df size: (995680, 22)\n",
      "After removing messed up rows: (995680, 22)\n",
      "After removing known bots df size: (991430, 22)\n",
      "After drop dup author/link id and NaN df size: (963703, 13)\n",
      "=== Parsing sus_usr_subreddits_4_new.csv (847003, 22) ===\n",
      "Original df size: (847003, 22)\n",
      "After removing messed up rows: (847003, 22)\n",
      "After removing known bots df size: (842111, 22)\n",
      "After drop dup author/link id and NaN df size: (823283, 13)\n",
      "=== Parsing sus_usr_subreddits_5_new.csv (237902, 22) ===\n",
      "Original df size: (237902, 22)\n",
      "After removing messed up rows: (237902, 22)\n",
      "After removing known bots df size: (236718, 22)\n",
      "After drop dup author/link id and NaN df size: (231396, 13)\n",
      "=== Parsing sus_usr_subreddits_6_new.csv (222456, 22) ===\n",
      "Original df size: (222456, 22)\n",
      "After removing messed up rows: (222456, 22)\n",
      "After removing known bots df size: (221501, 22)\n",
      "After drop dup author/link id and NaN df size: (217537, 13)\n",
      "=== Parsing sus_usr_subreddits_7_new.csv (469190, 22) ===\n",
      "Original df size: (469190, 22)\n",
      "After removing messed up rows: (469190, 22)\n",
      "After removing known bots df size: (462718, 22)\n",
      "After drop dup author/link id and NaN df size: (453447, 13)\n",
      "=== Parsing sus_usr_subreddits_8_new.csv (357967, 22) ===\n",
      "Original df size: (357967, 22)\n",
      "After removing messed up rows: (357967, 22)\n",
      "After removing known bots df size: (354158, 22)\n",
      "After drop dup author/link id and NaN df size: (346341, 13)\n",
      "=== Parsing sus_usr_subreddits_9_new.csv (668195, 22) ===\n",
      "Original df size: (668195, 22)\n",
      "After removing messed up rows: (668195, 22)\n",
      "After removing known bots df size: (663412, 22)\n",
      "After drop dup author/link id and NaN df size: (646323, 13)\n",
      "=== Parsing sus_usr_subreddits_posts.csv (2299936, 23) ===\n",
      "Original df size: (2299936, 23)\n",
      "After removing messed up rows: (1552986, 23)\n",
      "After removing known bots df size: (1552086, 23)\n",
      "After drop dup author/link id and NaN df size: (1215994, 13)\n"
     ]
    }
   ],
   "source": [
    "# Just get all nodes info, no need to make graph\n",
    "# Separate per csv\n",
    "for fn in os.listdir(data_path):\n",
    "    if fn.endswith(\".csv\"):\n",
    "        name = fn.split('.')[0]\n",
    "        type_sr = fn.split('_subreddits')[0]\n",
    "        df = pd.read_csv(os.path.join(data_path, fn))\n",
    "        print(f\"=== Parsing {fn} {df.shape} ===\")\n",
    "        df_clean = cleanup(df)\n",
    "        # analyze(df_clean)\n",
    "        mega_df = pd.concat([mega_df, df_clean])\n",
    "        # combo_category_df[type_sr] = pd.concat([combo_category_df[type_sr], df_clean])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Making SR DF ===\n",
      "Parsing political_center.csv\n",
      "Parsing political_communism.csv\n",
      "Parsing political_left.csv\n",
      "Parsing political_libertarian.csv\n",
      "Parsing political_other.csv\n",
      "Parsing political_right.csv\n",
      "Parsing popular_by_post_votes.csv\n",
      "Parsing popular_by_subscribers.csv\n",
      "Parsing sus_usrs_subreddits.csv\n",
      "=== Setting up for USR DF ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17096\\1416701085.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msr_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musr_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_node_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmega_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saving node attr to csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msr_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sr_node_attr.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0musr_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"usr_node_attr.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17096\\2888241650.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(df, no_set)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=== Setting up for USR DF ===\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'author'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'subreddit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'upvote_ratio'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'neg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'neu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pos'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'compound'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'author'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subreddit_set'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subreddit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'popular_sr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'popular'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Subreddit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subreddit_set'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'political_sr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'political'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Subreddit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subreddit_set'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'misc_sr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'misc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Subreddit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subreddit_set'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=== Making USR DF ===\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17096\\2888241650.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subreddit_set'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subreddit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3883\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3885\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3890\u001b[0m         \u001b[1;31m# We interpret tuples as collections only for non-MultiIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3945\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3946\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3948\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3949\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sr_df, usr_df, df = get_node_attr(mega_df, no_set=False)\n",
    "print(\"Saving node attr to csv\")\n",
    "sr_df.to_csv(\"sr_node_attr.csv\")\n",
    "usr_df.to_csv(\"usr_node_attr.csv\")\n",
    "df.to_csv(\"df_attr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>usr</th>\n",
       "      <th>top_sr</th>\n",
       "      <th>sr_set</th>\n",
       "      <th>num_sr</th>\n",
       "      <th>num_posts</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_popular_sr</th>\n",
       "      <th>num_political_sr</th>\n",
       "      <th>num_misc_sr</th>\n",
       "      <th>ratio_popular_sr</th>\n",
       "      <th>ratio_political_sr</th>\n",
       "      <th>ratio_misc_sr</th>\n",
       "      <th>avg_upvote_ratio</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_compound</th>\n",
       "      <th>node_type</th>\n",
       "      <th>is_sus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>*polhold00797</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>{'AskReddit'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.623900</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>*polhold01103</td>\n",
       "      <td>politics</td>\n",
       "      <td>{'politics'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>*polhold02060</td>\n",
       "      <td>science</td>\n",
       "      <td>{'science'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050750</td>\n",
       "      <td>0.838250</td>\n",
       "      <td>0.110750</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-------------------7</td>\n",
       "      <td>nyc</td>\n",
       "      <td>{'nyc'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>------------------GL</td>\n",
       "      <td>canadaguns</td>\n",
       "      <td>{'therewasanattempt', 'canadaguns'}</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059346</td>\n",
       "      <td>0.776769</td>\n",
       "      <td>0.163923</td>\n",
       "      <td>0.200531</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681289</th>\n",
       "      <td>1681289</td>\n",
       "      <td>zzzzzxx</td>\n",
       "      <td>nvidia</td>\n",
       "      <td>{'nvidia'}</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.022333</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.115333</td>\n",
       "      <td>0.600333</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681290</th>\n",
       "      <td>1681290</td>\n",
       "      <td>zzzzzzzz33bbbbbbb12</td>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>{'mildlyinteresting'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.648250</td>\n",
       "      <td>0.351750</td>\n",
       "      <td>0.367800</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681291</th>\n",
       "      <td>1681291</td>\n",
       "      <td>zzzzzzzzzz555</td>\n",
       "      <td>classiccars</td>\n",
       "      <td>{'classiccars'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.670500</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681292</th>\n",
       "      <td>1681292</td>\n",
       "      <td>zzzzzzzzzzzzzzz69</td>\n",
       "      <td>atheism</td>\n",
       "      <td>{'Firearms', 'atheism'}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.062333</td>\n",
       "      <td>0.841500</td>\n",
       "      <td>0.096167</td>\n",
       "      <td>-0.002833</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681293</th>\n",
       "      <td>1681293</td>\n",
       "      <td>zzzzzzzzzzzzzzzzspaf</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>{'worldnews', 'AskReddit', 'news'}</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020167</td>\n",
       "      <td>0.938833</td>\n",
       "      <td>0.040833</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1681294 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1                   usr             top_sr  \\\n",
       "0                   0         *polhold00797          AskReddit   \n",
       "1                   1         *polhold01103           politics   \n",
       "2                   2         *polhold02060            science   \n",
       "3                   3  -------------------7                nyc   \n",
       "4                   4  ------------------GL         canadaguns   \n",
       "...               ...                   ...                ...   \n",
       "1681289       1681289               zzzzzxx             nvidia   \n",
       "1681290       1681290   zzzzzzzz33bbbbbbb12  mildlyinteresting   \n",
       "1681291       1681291         zzzzzzzzzz555        classiccars   \n",
       "1681292       1681292     zzzzzzzzzzzzzzz69            atheism   \n",
       "1681293       1681293  zzzzzzzzzzzzzzzzspaf          AskReddit   \n",
       "\n",
       "                                      sr_set  num_sr  num_posts  num_comments  \\\n",
       "0                              {'AskReddit'}       1          0             1   \n",
       "1                               {'politics'}       1          0             1   \n",
       "2                                {'science'}       1          0             4   \n",
       "3                                    {'nyc'}       1          0             1   \n",
       "4        {'therewasanattempt', 'canadaguns'}       2          0            26   \n",
       "...                                      ...     ...        ...           ...   \n",
       "1681289                           {'nvidia'}       1          2             1   \n",
       "1681290                {'mildlyinteresting'}       1          0             4   \n",
       "1681291                      {'classiccars'}       1          0             1   \n",
       "1681292              {'Firearms', 'atheism'}       2          2             4   \n",
       "1681293   {'worldnews', 'AskReddit', 'news'}       3          0             6   \n",
       "\n",
       "         num_popular_sr  num_political_sr  num_misc_sr  ratio_popular_sr  \\\n",
       "0                     1                 0            0               1.0   \n",
       "1                     1                 0            0               1.0   \n",
       "2                     1                 0            0               1.0   \n",
       "3                     0                 0            1               0.0   \n",
       "4                     1                 0            1               0.5   \n",
       "...                 ...               ...          ...               ...   \n",
       "1681289               0                 0            1               0.0   \n",
       "1681290               1                 0            0               1.0   \n",
       "1681291               0                 0            1               0.0   \n",
       "1681292               0                 0            2               0.0   \n",
       "1681293               3                 0            0               1.0   \n",
       "\n",
       "         ratio_political_sr  ratio_misc_sr  avg_upvote_ratio   avg_neg  \\\n",
       "0                       0.0            0.0               NaN  0.000000   \n",
       "1                       0.0            0.0               NaN  0.000000   \n",
       "2                       0.0            0.0               NaN  0.050750   \n",
       "3                       0.0            1.0               NaN  0.076000   \n",
       "4                       0.0            0.5               NaN  0.059346   \n",
       "...                     ...            ...               ...       ...   \n",
       "1681289                 0.0            1.0               0.3  0.022333   \n",
       "1681290                 0.0            0.0               NaN  0.000000   \n",
       "1681291                 0.0            1.0               NaN  0.391000   \n",
       "1681292                 0.0            1.0               0.9  0.062333   \n",
       "1681293                 0.0            0.0               NaN  0.020167   \n",
       "\n",
       "          avg_neu   avg_pos  avg_compound node_type  is_sus  \n",
       "0        0.000000  1.000000      0.623900      user   False  \n",
       "1        1.000000  0.000000      0.000000      user   False  \n",
       "2        0.838250  0.110750      0.318000      user   False  \n",
       "3        0.665000  0.260000      0.796400      user   False  \n",
       "4        0.776769  0.163923      0.200531      user   False  \n",
       "...           ...       ...           ...       ...     ...  \n",
       "1681289  0.862000  0.115333      0.600333      user   False  \n",
       "1681290  0.648250  0.351750      0.367800      user   False  \n",
       "1681291  0.609000  0.000000     -0.670500      user   False  \n",
       "1681292  0.841500  0.096167     -0.002833      user   False  \n",
       "1681293  0.938833  0.040833      0.058200      user   False  \n",
       "\n",
       "[1681294 rows x 20 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr_df = pd.read_csv(\"usr_node_attr.csv\")\n",
    "usr_df = label_sus_usr(usr_df)\n",
    "usr_df.to_csv(\"usr_node_attr.csv\")\n",
    "usr_df.drop(labels=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "usr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>usr</th>\n",
       "      <th>top_sr</th>\n",
       "      <th>sr_set</th>\n",
       "      <th>num_sr</th>\n",
       "      <th>num_posts</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_popular_sr</th>\n",
       "      <th>num_political_sr</th>\n",
       "      <th>num_misc_sr</th>\n",
       "      <th>ratio_popular_sr</th>\n",
       "      <th>ratio_political_sr</th>\n",
       "      <th>ratio_misc_sr</th>\n",
       "      <th>avg_upvote_ratio</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_compound</th>\n",
       "      <th>node_type</th>\n",
       "      <th>is_sus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7555</th>\n",
       "      <td>7555</td>\n",
       "      <td>1488reasons</td>\n",
       "      <td>newzealand</td>\n",
       "      <td>{'newzealand', 'NorthAmerican', 'environment'}</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.112120</td>\n",
       "      <td>0.796858</td>\n",
       "      <td>0.090995</td>\n",
       "      <td>-0.064072</td>\n",
       "      <td>user</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29722</th>\n",
       "      <td>29722</td>\n",
       "      <td>Abena_Tau</td>\n",
       "      <td>Blackpeople</td>\n",
       "      <td>{'blackculture', 'Blackpeople', 'worldnews', '...</td>\n",
       "      <td>19</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.909429</td>\n",
       "      <td>0.208629</td>\n",
       "      <td>0.743514</td>\n",
       "      <td>0.047857</td>\n",
       "      <td>-0.283026</td>\n",
       "      <td>user</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39784</th>\n",
       "      <td>39784</td>\n",
       "      <td>AdoraronDoomworker</td>\n",
       "      <td>funny</td>\n",
       "      <td>{'pics', 'funny', 'gifs', 'Bad_Cop_No_Donut'}</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.761250</td>\n",
       "      <td>0.208500</td>\n",
       "      <td>0.661750</td>\n",
       "      <td>0.129875</td>\n",
       "      <td>-0.093150</td>\n",
       "      <td>user</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43712</th>\n",
       "      <td>43712</td>\n",
       "      <td>AgaluneMalordred</td>\n",
       "      <td>Bad_Cop_No_Donut</td>\n",
       "      <td>{'PoliticalHumor', 'Bad_Cop_No_Donut', 'funny'...</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.964444</td>\n",
       "      <td>0.066889</td>\n",
       "      <td>0.772333</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.097422</td>\n",
       "      <td>user</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43713</th>\n",
       "      <td>43713</td>\n",
       "      <td>AgamagelvTozshura</td>\n",
       "      <td>gifs</td>\n",
       "      <td>{'obama', 'PoliticalHumor', 'MRW', 'WTF', 'tod...</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.652105</td>\n",
       "      <td>0.125789</td>\n",
       "      <td>0.804947</td>\n",
       "      <td>0.069263</td>\n",
       "      <td>-0.099168</td>\n",
       "      <td>user</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640892</th>\n",
       "      <td>1640892</td>\n",
       "      <td>vsrruslan</td>\n",
       "      <td>police</td>\n",
       "      <td>{'test', 'police'}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>user</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640893</th>\n",
       "      <td>1640893</td>\n",
       "      <td>vsruslan</td>\n",
       "      <td>test</td>\n",
       "      <td>{'test', 'elections', 'lgbt'}</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.924500</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.095450</td>\n",
       "      <td>user</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641937</th>\n",
       "      <td>1641937</td>\n",
       "      <td>wadeharriot</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>{'television', 'worldnews', 'promos', 'Jokes',...</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.856250</td>\n",
       "      <td>0.058560</td>\n",
       "      <td>0.798080</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>0.151320</td>\n",
       "      <td>user</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643104</th>\n",
       "      <td>1643104</td>\n",
       "      <td>walterwhite1962</td>\n",
       "      <td>snapleaks</td>\n",
       "      <td>{'snapleaks'}</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>user</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663126</th>\n",
       "      <td>1663126</td>\n",
       "      <td>xameg</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>{'RetroFuturism', 'pcmasterrace', 'MechanicalK...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.056643</td>\n",
       "      <td>0.809143</td>\n",
       "      <td>0.134214</td>\n",
       "      <td>0.203493</td>\n",
       "      <td>user</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>326 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1                 usr            top_sr  \\\n",
       "7555             7555         1488reasons        newzealand   \n",
       "29722           29722           Abena_Tau       Blackpeople   \n",
       "39784           39784  AdoraronDoomworker             funny   \n",
       "43712           43712    AgaluneMalordred  Bad_Cop_No_Donut   \n",
       "43713           43713   AgamagelvTozshura              gifs   \n",
       "...               ...                 ...               ...   \n",
       "1640892       1640892           vsrruslan            police   \n",
       "1640893       1640893            vsruslan              test   \n",
       "1641937       1641937         wadeharriot      Conservative   \n",
       "1643104       1643104     walterwhite1962         snapleaks   \n",
       "1663126       1663126               xameg      pcmasterrace   \n",
       "\n",
       "                                                    sr_set  num_sr  num_posts  \\\n",
       "7555        {'newzealand', 'NorthAmerican', 'environment'}       3          0   \n",
       "29722    {'blackculture', 'Blackpeople', 'worldnews', '...      19         35   \n",
       "39784        {'pics', 'funny', 'gifs', 'Bad_Cop_No_Donut'}       4          8   \n",
       "43712    {'PoliticalHumor', 'Bad_Cop_No_Donut', 'funny'...       5          9   \n",
       "43713    {'obama', 'PoliticalHumor', 'MRW', 'WTF', 'tod...       8         19   \n",
       "...                                                    ...     ...        ...   \n",
       "1640892                                 {'test', 'police'}       2          2   \n",
       "1640893                      {'test', 'elections', 'lgbt'}       3          4   \n",
       "1641937  {'television', 'worldnews', 'promos', 'Jokes',...      14          8   \n",
       "1643104                                      {'snapleaks'}       1          1   \n",
       "1663126  {'RetroFuturism', 'pcmasterrace', 'MechanicalK...       7          1   \n",
       "\n",
       "         num_comments  num_popular_sr  num_political_sr  num_misc_sr  \\\n",
       "7555              183               0                 0            3   \n",
       "29722               0               2                 1           15   \n",
       "39784               0               3                 1            0   \n",
       "43712               0               2                 2            1   \n",
       "43713               0               4                 2            2   \n",
       "...               ...             ...               ...          ...   \n",
       "1640892             0               0                 0            2   \n",
       "1640893             0               0                 0            3   \n",
       "1641937            17              10                 2            1   \n",
       "1643104             0               0                 0            1   \n",
       "1663126            13               1                 0            6   \n",
       "\n",
       "         ratio_popular_sr  ratio_political_sr  ratio_misc_sr  \\\n",
       "7555             0.000000            0.000000       1.000000   \n",
       "29722            0.105263            0.052632       0.789474   \n",
       "39784            0.750000            0.250000       0.000000   \n",
       "43712            0.400000            0.400000       0.200000   \n",
       "43713            0.500000            0.250000       0.250000   \n",
       "...                   ...                 ...            ...   \n",
       "1640892          0.000000            0.000000       1.000000   \n",
       "1640893          0.000000            0.000000       1.000000   \n",
       "1641937          0.714286            0.142857       0.071429   \n",
       "1643104          0.000000            0.000000       1.000000   \n",
       "1663126          0.142857            0.000000       0.857143   \n",
       "\n",
       "         avg_upvote_ratio   avg_neg   avg_neu   avg_pos  avg_compound  \\\n",
       "7555                  NaN  0.112120  0.796858  0.090995     -0.064072   \n",
       "29722            0.909429  0.208629  0.743514  0.047857     -0.283026   \n",
       "39784            0.761250  0.208500  0.661750  0.129875     -0.093150   \n",
       "43712            0.964444  0.066889  0.772333  0.161000      0.097422   \n",
       "43713            0.652105  0.125789  0.804947  0.069263     -0.099168   \n",
       "...                   ...       ...       ...       ...           ...   \n",
       "1640892          0.665000  0.000000  1.000000  0.000000      0.000000   \n",
       "1640893          1.000000  0.000000  0.924500  0.075500      0.095450   \n",
       "1641937          0.856250  0.058560  0.798080  0.143400      0.151320   \n",
       "1643104          0.850000  0.486000  0.000000  0.514000      0.051600   \n",
       "1663126          0.570000  0.056643  0.809143  0.134214      0.203493   \n",
       "\n",
       "        node_type  is_sus  \n",
       "7555         user    True  \n",
       "29722        user    True  \n",
       "39784        user    True  \n",
       "43712        user    True  \n",
       "43713        user    True  \n",
       "...           ...     ...  \n",
       "1640892      user    True  \n",
       "1640893      user    True  \n",
       "1641937      user    True  \n",
       "1643104      user    True  \n",
       "1663126      user    True  \n",
       "\n",
       "[326 rows x 20 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr_df[usr_df[\"is_sus\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usr</th>\n",
       "      <th>top_sr</th>\n",
       "      <th>sr_set</th>\n",
       "      <th>num_sr</th>\n",
       "      <th>num_posts</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_popular_sr</th>\n",
       "      <th>num_political_sr</th>\n",
       "      <th>num_misc_sr</th>\n",
       "      <th>ratio_popular_sr</th>\n",
       "      <th>ratio_political_sr</th>\n",
       "      <th>ratio_misc_sr</th>\n",
       "      <th>avg_upvote_ratio</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_compound</th>\n",
       "      <th>node_type</th>\n",
       "      <th>is_sus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*polhold00797</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>{'AskReddit'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.623900</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*polhold01103</td>\n",
       "      <td>politics</td>\n",
       "      <td>{'politics'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*polhold02060</td>\n",
       "      <td>science</td>\n",
       "      <td>{'science'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050750</td>\n",
       "      <td>0.838250</td>\n",
       "      <td>0.110750</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-------------------7</td>\n",
       "      <td>nyc</td>\n",
       "      <td>{'nyc'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>------------------GL</td>\n",
       "      <td>canadaguns</td>\n",
       "      <td>{'therewasanattempt', 'canadaguns'}</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059346</td>\n",
       "      <td>0.776769</td>\n",
       "      <td>0.163923</td>\n",
       "      <td>0.200531</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681289</th>\n",
       "      <td>zzzzzxx</td>\n",
       "      <td>nvidia</td>\n",
       "      <td>{'nvidia'}</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.022333</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.115333</td>\n",
       "      <td>0.600333</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681290</th>\n",
       "      <td>zzzzzzzz33bbbbbbb12</td>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>{'mildlyinteresting'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.648250</td>\n",
       "      <td>0.351750</td>\n",
       "      <td>0.367800</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681291</th>\n",
       "      <td>zzzzzzzzzz555</td>\n",
       "      <td>classiccars</td>\n",
       "      <td>{'classiccars'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.670500</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681292</th>\n",
       "      <td>zzzzzzzzzzzzzzz69</td>\n",
       "      <td>atheism</td>\n",
       "      <td>{'Firearms', 'atheism'}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.062333</td>\n",
       "      <td>0.841500</td>\n",
       "      <td>0.096167</td>\n",
       "      <td>-0.002833</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681293</th>\n",
       "      <td>zzzzzzzzzzzzzzzzspaf</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>{'worldnews', 'AskReddit', 'news'}</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020167</td>\n",
       "      <td>0.938833</td>\n",
       "      <td>0.040833</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>user</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1681294 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          usr             top_sr  \\\n",
       "0               *polhold00797          AskReddit   \n",
       "1               *polhold01103           politics   \n",
       "2               *polhold02060            science   \n",
       "3        -------------------7                nyc   \n",
       "4        ------------------GL         canadaguns   \n",
       "...                       ...                ...   \n",
       "1681289               zzzzzxx             nvidia   \n",
       "1681290   zzzzzzzz33bbbbbbb12  mildlyinteresting   \n",
       "1681291         zzzzzzzzzz555        classiccars   \n",
       "1681292     zzzzzzzzzzzzzzz69            atheism   \n",
       "1681293  zzzzzzzzzzzzzzzzspaf          AskReddit   \n",
       "\n",
       "                                      sr_set  num_sr  num_posts  num_comments  \\\n",
       "0                              {'AskReddit'}       1          0             1   \n",
       "1                               {'politics'}       1          0             1   \n",
       "2                                {'science'}       1          0             4   \n",
       "3                                    {'nyc'}       1          0             1   \n",
       "4        {'therewasanattempt', 'canadaguns'}       2          0            26   \n",
       "...                                      ...     ...        ...           ...   \n",
       "1681289                           {'nvidia'}       1          2             1   \n",
       "1681290                {'mildlyinteresting'}       1          0             4   \n",
       "1681291                      {'classiccars'}       1          0             1   \n",
       "1681292              {'Firearms', 'atheism'}       2          2             4   \n",
       "1681293   {'worldnews', 'AskReddit', 'news'}       3          0             6   \n",
       "\n",
       "         num_popular_sr  num_political_sr  num_misc_sr  ratio_popular_sr  \\\n",
       "0                     1                 0            0               1.0   \n",
       "1                     1                 0            0               1.0   \n",
       "2                     1                 0            0               1.0   \n",
       "3                     0                 0            1               0.0   \n",
       "4                     1                 0            1               0.5   \n",
       "...                 ...               ...          ...               ...   \n",
       "1681289               0                 0            1               0.0   \n",
       "1681290               1                 0            0               1.0   \n",
       "1681291               0                 0            1               0.0   \n",
       "1681292               0                 0            2               0.0   \n",
       "1681293               3                 0            0               1.0   \n",
       "\n",
       "         ratio_political_sr  ratio_misc_sr  avg_upvote_ratio   avg_neg  \\\n",
       "0                       0.0            0.0               NaN  0.000000   \n",
       "1                       0.0            0.0               NaN  0.000000   \n",
       "2                       0.0            0.0               NaN  0.050750   \n",
       "3                       0.0            1.0               NaN  0.076000   \n",
       "4                       0.0            0.5               NaN  0.059346   \n",
       "...                     ...            ...               ...       ...   \n",
       "1681289                 0.0            1.0               0.3  0.022333   \n",
       "1681290                 0.0            0.0               NaN  0.000000   \n",
       "1681291                 0.0            1.0               NaN  0.391000   \n",
       "1681292                 0.0            1.0               0.9  0.062333   \n",
       "1681293                 0.0            0.0               NaN  0.020167   \n",
       "\n",
       "          avg_neu   avg_pos  avg_compound node_type  is_sus  \n",
       "0        0.000000  1.000000      0.623900      user   False  \n",
       "1        1.000000  0.000000      0.000000      user   False  \n",
       "2        0.838250  0.110750      0.318000      user   False  \n",
       "3        0.665000  0.260000      0.796400      user   False  \n",
       "4        0.776769  0.163923      0.200531      user   False  \n",
       "...           ...       ...           ...       ...     ...  \n",
       "1681289  0.862000  0.115333      0.600333      user   False  \n",
       "1681290  0.648250  0.351750      0.367800      user   False  \n",
       "1681291  0.609000  0.000000     -0.670500      user   False  \n",
       "1681292  0.841500  0.096167     -0.002833      user   False  \n",
       "1681293  0.938833  0.040833      0.058200      user   False  \n",
       "\n",
       "[1681294 rows x 19 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr_df = pd.read_csv(\"usr_node_attr.csv\")\n",
    "try:\n",
    "    usr_df.drop(labels=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "usr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usr',\n",
       " 'top_sr',\n",
       " 'sr_set',\n",
       " 'num_sr',\n",
       " 'num_posts',\n",
       " 'num_comments',\n",
       " 'num_popular_sr',\n",
       " 'num_political_sr',\n",
       " 'num_misc_sr',\n",
       " 'ratio_popular_sr',\n",
       " 'ratio_political_sr',\n",
       " 'ratio_misc_sr',\n",
       " 'avg_upvote_ratio',\n",
       " 'avg_neg',\n",
       " 'avg_neu',\n",
       " 'avg_pos',\n",
       " 'avg_compound',\n",
       " 'is_sus']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr_cols = [col for col in usr_df.columns]\n",
    "usr_cols.remove(\"node_type\")\n",
    "usr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Title</th>\n",
       "      <th>Public Description</th>\n",
       "      <th>Description</th>\n",
       "      <th>topic_type</th>\n",
       "      <th>node_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NeutralPolitics</td>\n",
       "      <td>Neutral Politics: Evidence. Logic. Respect.</td>\n",
       "      <td>Neutral Politics is a community dedicated to e...</td>\n",
       "      <td>##What is Neutral Politics?\\n\\nNeutral Politic...</td>\n",
       "      <td>political</td>\n",
       "      <td>subreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Centrist</td>\n",
       "      <td>Centrist Reddit</td>\n",
       "      <td>A subreddit for those who gravitate to the mid...</td>\n",
       "      <td>Finally, a Reddit for those of us in the middl...</td>\n",
       "      <td>political</td>\n",
       "      <td>subreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ModeratePolitics</td>\n",
       "      <td>Restore Sanity in Politics!</td>\n",
       "      <td>This is NOT a politically moderate subreddit! ...</td>\n",
       "      <td>Started by u/sockthepuppetry in 2011, this sub...</td>\n",
       "      <td>political</td>\n",
       "      <td>subreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PeoplesParty</td>\n",
       "      <td>People's Party</td>\n",
       "      <td>A sub to discuss Maxime Bernier's People's Par...</td>\n",
       "      <td>Welcome to r/PeoplesParty. We're a subreddit d...</td>\n",
       "      <td>political</td>\n",
       "      <td>subreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Communism</td>\n",
       "      <td>COMMUNISM</td>\n",
       "      <td>For the theory and practice of Marxism.</td>\n",
       "      <td>### [Please read the rules **before** posting....</td>\n",
       "      <td>political</td>\n",
       "      <td>subreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>redditrequest</td>\n",
       "      <td>RedditRequest - Adopt an unmoderated community!</td>\n",
       "      <td>This subreddit is for requesting moderation pr...</td>\n",
       "      <td>**The current review time for requests is 5 da...</td>\n",
       "      <td>misc</td>\n",
       "      <td>subreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>toosoon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>misc</td>\n",
       "      <td>subreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>dogemarket</td>\n",
       "      <td>DogeMarket</td>\n",
       "      <td>buy and sell things with dogecoin!\\n\\nRead the...</td>\n",
       "      <td>&gt; **Subreddit Style**\\n\\n&gt; You are not using t...</td>\n",
       "      <td>misc</td>\n",
       "      <td>subreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>dogemining</td>\n",
       "      <td>Dogecoin Mining</td>\n",
       "      <td>Dogecoin Mining Subreddit. Many digs. Much Dog...</td>\n",
       "      <td>### Guides\\n* [General Mining Guide](/r/dogemi...</td>\n",
       "      <td>misc</td>\n",
       "      <td>subreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>provenlands</td>\n",
       "      <td>A Sci-Fi Sandbox Roguelike</td>\n",
       "      <td>A community for fans of the upcoming game Prov...</td>\n",
       "      <td>[Proven Lands Website](http://provenlands.com/...</td>\n",
       "      <td>misc</td>\n",
       "      <td>subreddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1354 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Subreddit                                            Title  \\\n",
       "0      NeutralPolitics      Neutral Politics: Evidence. Logic. Respect.   \n",
       "1             Centrist                                  Centrist Reddit   \n",
       "2     ModeratePolitics                      Restore Sanity in Politics!   \n",
       "3         PeoplesParty                                   People's Party   \n",
       "4            Communism                                        COMMUNISM   \n",
       "...                ...                                              ...   \n",
       "1349     redditrequest  RedditRequest - Adopt an unmoderated community!   \n",
       "1350           toosoon                                              NaN   \n",
       "1351        dogemarket                                       DogeMarket   \n",
       "1352        dogemining                                  Dogecoin Mining   \n",
       "1353       provenlands                       A Sci-Fi Sandbox Roguelike   \n",
       "\n",
       "                                     Public Description  \\\n",
       "0     Neutral Politics is a community dedicated to e...   \n",
       "1     A subreddit for those who gravitate to the mid...   \n",
       "2     This is NOT a politically moderate subreddit! ...   \n",
       "3     A sub to discuss Maxime Bernier's People's Par...   \n",
       "4               For the theory and practice of Marxism.   \n",
       "...                                                 ...   \n",
       "1349  This subreddit is for requesting moderation pr...   \n",
       "1350                                                NaN   \n",
       "1351  buy and sell things with dogecoin!\\n\\nRead the...   \n",
       "1352  Dogecoin Mining Subreddit. Many digs. Much Dog...   \n",
       "1353  A community for fans of the upcoming game Prov...   \n",
       "\n",
       "                                            Description topic_type  node_type  \n",
       "0     ##What is Neutral Politics?\\n\\nNeutral Politic...  political  subreddit  \n",
       "1     Finally, a Reddit for those of us in the middl...  political  subreddit  \n",
       "2     Started by u/sockthepuppetry in 2011, this sub...  political  subreddit  \n",
       "3     Welcome to r/PeoplesParty. We're a subreddit d...  political  subreddit  \n",
       "4     ### [Please read the rules **before** posting....  political  subreddit  \n",
       "...                                                 ...        ...        ...  \n",
       "1349  **The current review time for requests is 5 da...       misc  subreddit  \n",
       "1350                                                NaN       misc  subreddit  \n",
       "1351  > **Subreddit Style**\\n\\n> You are not using t...       misc  subreddit  \n",
       "1352  ### Guides\\n* [General Mining Guide](/r/dogemi...       misc  subreddit  \n",
       "1353  [Proven Lands Website](http://provenlands.com/...       misc  subreddit  \n",
       "\n",
       "[1354 rows x 6 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_df = pd.read_csv(\"sr_node_attr.csv\")\n",
    "try:\n",
    "    sr_df.drop(labels=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "sr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subreddit', 'Title', 'Public Description', 'Description', 'topic_type']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_cols = [col for col in sr_df.columns]\n",
    "sr_cols.remove(\"node_type\")\n",
    "sr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['node_type',\n",
       " 'usr',\n",
       " 'top_sr',\n",
       " 'sr_set',\n",
       " 'num_sr',\n",
       " 'num_posts',\n",
       " 'num_comments',\n",
       " 'num_popular_sr',\n",
       " 'num_political_sr',\n",
       " 'num_misc_sr',\n",
       " 'ratio_popular_sr',\n",
       " 'ratio_political_sr',\n",
       " 'ratio_misc_sr',\n",
       " 'avg_upvote_ratio',\n",
       " 'avg_neg',\n",
       " 'avg_neu',\n",
       " 'avg_pos',\n",
       " 'avg_compound',\n",
       " 'is_sus',\n",
       " 'Subreddit',\n",
       " 'Title',\n",
       " 'Public Description',\n",
       " 'Description',\n",
       " 'topic_type']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_cols = ['node_type'] + usr_cols + sr_cols\n",
    "ordered_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_type</th>\n",
       "      <th>usr</th>\n",
       "      <th>top_sr</th>\n",
       "      <th>sr_set</th>\n",
       "      <th>num_sr</th>\n",
       "      <th>num_posts</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_popular_sr</th>\n",
       "      <th>num_political_sr</th>\n",
       "      <th>num_misc_sr</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_compound</th>\n",
       "      <th>is_sus</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Title</th>\n",
       "      <th>Public Description</th>\n",
       "      <th>Description</th>\n",
       "      <th>topic_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NeutralPolitics</td>\n",
       "      <td>Neutral Politics: Evidence. Logic. Respect.</td>\n",
       "      <td>Neutral Politics is a community dedicated to e...</td>\n",
       "      <td>##What is Neutral Politics?\\n\\nNeutral Politic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Centrist</td>\n",
       "      <td>Centrist Reddit</td>\n",
       "      <td>A subreddit for those who gravitate to the mid...</td>\n",
       "      <td>Finally, a Reddit for those of us in the middl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ModeratePolitics</td>\n",
       "      <td>Restore Sanity in Politics!</td>\n",
       "      <td>This is NOT a politically moderate subreddit! ...</td>\n",
       "      <td>Started by u/sockthepuppetry in 2011, this sub...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PeoplesParty</td>\n",
       "      <td>People's Party</td>\n",
       "      <td>A sub to discuss Maxime Bernier's People's Par...</td>\n",
       "      <td>Welcome to r/PeoplesParty. We're a subreddit d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Communism</td>\n",
       "      <td>COMMUNISM</td>\n",
       "      <td>For the theory and practice of Marxism.</td>\n",
       "      <td>### [Please read the rules **before** posting....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>redditrequest</td>\n",
       "      <td>RedditRequest - Adopt an unmoderated community!</td>\n",
       "      <td>This subreddit is for requesting moderation pr...</td>\n",
       "      <td>**The current review time for requests is 5 da...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>toosoon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dogemarket</td>\n",
       "      <td>DogeMarket</td>\n",
       "      <td>buy and sell things with dogecoin!\\n\\nRead the...</td>\n",
       "      <td>&gt; **Subreddit Style**\\n\\n&gt; You are not using t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dogemining</td>\n",
       "      <td>Dogecoin Mining</td>\n",
       "      <td>Dogecoin Mining Subreddit. Many digs. Much Dog...</td>\n",
       "      <td>### Guides\\n* [General Mining Guide](/r/dogemi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>provenlands</td>\n",
       "      <td>A Sci-Fi Sandbox Roguelike</td>\n",
       "      <td>A community for fans of the upcoming game Prov...</td>\n",
       "      <td>[Proven Lands Website](http://provenlands.com/...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1354 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      node_type  usr  top_sr  sr_set  num_sr  num_posts  num_comments  \\\n",
       "0             1    0       0       0       0          0             0   \n",
       "1             1    0       0       0       0          0             0   \n",
       "2             1    0       0       0       0          0             0   \n",
       "3             1    0       0       0       0          0             0   \n",
       "4             1    0       0       0       0          0             0   \n",
       "...         ...  ...     ...     ...     ...        ...           ...   \n",
       "1349          1    0       0       0       0          0             0   \n",
       "1350          1    0       0       0       0          0             0   \n",
       "1351          1    0       0       0       0          0             0   \n",
       "1352          1    0       0       0       0          0             0   \n",
       "1353          1    0       0       0       0          0             0   \n",
       "\n",
       "      num_popular_sr  num_political_sr  num_misc_sr  ...  avg_neg  avg_neu  \\\n",
       "0                  0                 0            0  ...        0        0   \n",
       "1                  0                 0            0  ...        0        0   \n",
       "2                  0                 0            0  ...        0        0   \n",
       "3                  0                 0            0  ...        0        0   \n",
       "4                  0                 0            0  ...        0        0   \n",
       "...              ...               ...          ...  ...      ...      ...   \n",
       "1349               0                 0            0  ...        0        0   \n",
       "1350               0                 0            0  ...        0        0   \n",
       "1351               0                 0            0  ...        0        0   \n",
       "1352               0                 0            0  ...        0        0   \n",
       "1353               0                 0            0  ...        0        0   \n",
       "\n",
       "      avg_pos  avg_compound  is_sus         Subreddit  \\\n",
       "0           0             0       0   NeutralPolitics   \n",
       "1           0             0       0          Centrist   \n",
       "2           0             0       0  ModeratePolitics   \n",
       "3           0             0       0      PeoplesParty   \n",
       "4           0             0       0         Communism   \n",
       "...       ...           ...     ...               ...   \n",
       "1349        0             0       0     redditrequest   \n",
       "1350        0             0       0           toosoon   \n",
       "1351        0             0       0        dogemarket   \n",
       "1352        0             0       0        dogemining   \n",
       "1353        0             0       0       provenlands   \n",
       "\n",
       "                                                Title  \\\n",
       "0         Neutral Politics: Evidence. Logic. Respect.   \n",
       "1                                     Centrist Reddit   \n",
       "2                         Restore Sanity in Politics!   \n",
       "3                                      People's Party   \n",
       "4                                           COMMUNISM   \n",
       "...                                               ...   \n",
       "1349  RedditRequest - Adopt an unmoderated community!   \n",
       "1350                                              NaN   \n",
       "1351                                       DogeMarket   \n",
       "1352                                  Dogecoin Mining   \n",
       "1353                       A Sci-Fi Sandbox Roguelike   \n",
       "\n",
       "                                     Public Description  \\\n",
       "0     Neutral Politics is a community dedicated to e...   \n",
       "1     A subreddit for those who gravitate to the mid...   \n",
       "2     This is NOT a politically moderate subreddit! ...   \n",
       "3     A sub to discuss Maxime Bernier's People's Par...   \n",
       "4               For the theory and practice of Marxism.   \n",
       "...                                                 ...   \n",
       "1349  This subreddit is for requesting moderation pr...   \n",
       "1350                                                NaN   \n",
       "1351  buy and sell things with dogecoin!\\n\\nRead the...   \n",
       "1352  Dogecoin Mining Subreddit. Many digs. Much Dog...   \n",
       "1353  A community for fans of the upcoming game Prov...   \n",
       "\n",
       "                                            Description topic_type  \n",
       "0     ##What is Neutral Politics?\\n\\nNeutral Politic...          0  \n",
       "1     Finally, a Reddit for those of us in the middl...          0  \n",
       "2     Started by u/sockthepuppetry in 2011, this sub...          0  \n",
       "3     Welcome to r/PeoplesParty. We're a subreddit d...          0  \n",
       "4     ### [Please read the rules **before** posting....          0  \n",
       "...                                                 ...        ...  \n",
       "1349  **The current review time for requests is 5 da...          2  \n",
       "1350                                                NaN          2  \n",
       "1351  > **Subreddit Style**\\n\\n> You are not using t...          2  \n",
       "1352  ### Guides\\n* [General Mining Guide](/r/dogemi...          2  \n",
       "1353  [Proven Lands Website](http://provenlands.com/...          2  \n",
       "\n",
       "[1354 rows x 24 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Add all attributes to both node types, but values will be NaN for the non-node type attributes\n",
    "for col in usr_cols:\n",
    "    if col not in sr_df.columns:\n",
    "        sr_df[col] = 0 \n",
    "sr_df['topic_type'].replace('political', 0, inplace=True)\n",
    "sr_df['topic_type'].replace('popular', 1, inplace=True)\n",
    "sr_df['topic_type'].replace('misc', 2, inplace=True)\n",
    "sr_df['node_type'].replace('user', 0, inplace=True)\n",
    "sr_df['node_type'].replace('subreddit', 1, inplace=True)\n",
    "sr_df = sr_df.reindex(ordered_cols, axis=1)\n",
    "sr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_type</th>\n",
       "      <th>usr</th>\n",
       "      <th>top_sr</th>\n",
       "      <th>sr_set</th>\n",
       "      <th>num_sr</th>\n",
       "      <th>num_posts</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_popular_sr</th>\n",
       "      <th>num_political_sr</th>\n",
       "      <th>num_misc_sr</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_compound</th>\n",
       "      <th>is_sus</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Title</th>\n",
       "      <th>Public Description</th>\n",
       "      <th>Description</th>\n",
       "      <th>topic_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>*polhold00797</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>{'AskReddit'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.623900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>*polhold01103</td>\n",
       "      <td>politics</td>\n",
       "      <td>{'politics'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>*polhold02060</td>\n",
       "      <td>science</td>\n",
       "      <td>{'science'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050750</td>\n",
       "      <td>0.838250</td>\n",
       "      <td>0.110750</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-------------------7</td>\n",
       "      <td>nyc</td>\n",
       "      <td>{'nyc'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>------------------GL</td>\n",
       "      <td>canadaguns</td>\n",
       "      <td>{'therewasanattempt', 'canadaguns'}</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059346</td>\n",
       "      <td>0.776769</td>\n",
       "      <td>0.163923</td>\n",
       "      <td>0.200531</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681289</th>\n",
       "      <td>0</td>\n",
       "      <td>zzzzzxx</td>\n",
       "      <td>nvidia</td>\n",
       "      <td>{'nvidia'}</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022333</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.115333</td>\n",
       "      <td>0.600333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681290</th>\n",
       "      <td>0</td>\n",
       "      <td>zzzzzzzz33bbbbbbb12</td>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>{'mildlyinteresting'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.648250</td>\n",
       "      <td>0.351750</td>\n",
       "      <td>0.367800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681291</th>\n",
       "      <td>0</td>\n",
       "      <td>zzzzzzzzzz555</td>\n",
       "      <td>classiccars</td>\n",
       "      <td>{'classiccars'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.670500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681292</th>\n",
       "      <td>0</td>\n",
       "      <td>zzzzzzzzzzzzzzz69</td>\n",
       "      <td>atheism</td>\n",
       "      <td>{'Firearms', 'atheism'}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062333</td>\n",
       "      <td>0.841500</td>\n",
       "      <td>0.096167</td>\n",
       "      <td>-0.002833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681293</th>\n",
       "      <td>0</td>\n",
       "      <td>zzzzzzzzzzzzzzzzspaf</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>{'worldnews', 'AskReddit', 'news'}</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020167</td>\n",
       "      <td>0.938833</td>\n",
       "      <td>0.040833</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1681294 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         node_type                   usr             top_sr  \\\n",
       "0                0         *polhold00797          AskReddit   \n",
       "1                0         *polhold01103           politics   \n",
       "2                0         *polhold02060            science   \n",
       "3                0  -------------------7                nyc   \n",
       "4                0  ------------------GL         canadaguns   \n",
       "...            ...                   ...                ...   \n",
       "1681289          0               zzzzzxx             nvidia   \n",
       "1681290          0   zzzzzzzz33bbbbbbb12  mildlyinteresting   \n",
       "1681291          0         zzzzzzzzzz555        classiccars   \n",
       "1681292          0     zzzzzzzzzzzzzzz69            atheism   \n",
       "1681293          0  zzzzzzzzzzzzzzzzspaf          AskReddit   \n",
       "\n",
       "                                      sr_set  num_sr  num_posts  num_comments  \\\n",
       "0                              {'AskReddit'}       1          0             1   \n",
       "1                               {'politics'}       1          0             1   \n",
       "2                                {'science'}       1          0             4   \n",
       "3                                    {'nyc'}       1          0             1   \n",
       "4        {'therewasanattempt', 'canadaguns'}       2          0            26   \n",
       "...                                      ...     ...        ...           ...   \n",
       "1681289                           {'nvidia'}       1          2             1   \n",
       "1681290                {'mildlyinteresting'}       1          0             4   \n",
       "1681291                      {'classiccars'}       1          0             1   \n",
       "1681292              {'Firearms', 'atheism'}       2          2             4   \n",
       "1681293   {'worldnews', 'AskReddit', 'news'}       3          0             6   \n",
       "\n",
       "         num_popular_sr  num_political_sr  num_misc_sr  ...   avg_neg  \\\n",
       "0                     1                 0            0  ...  0.000000   \n",
       "1                     1                 0            0  ...  0.000000   \n",
       "2                     1                 0            0  ...  0.050750   \n",
       "3                     0                 0            1  ...  0.076000   \n",
       "4                     1                 0            1  ...  0.059346   \n",
       "...                 ...               ...          ...  ...       ...   \n",
       "1681289               0                 0            1  ...  0.022333   \n",
       "1681290               1                 0            0  ...  0.000000   \n",
       "1681291               0                 0            1  ...  0.391000   \n",
       "1681292               0                 0            2  ...  0.062333   \n",
       "1681293               3                 0            0  ...  0.020167   \n",
       "\n",
       "          avg_neu   avg_pos  avg_compound  is_sus  Subreddit  Title  \\\n",
       "0        0.000000  1.000000      0.623900       0          0      0   \n",
       "1        1.000000  0.000000      0.000000       0          0      0   \n",
       "2        0.838250  0.110750      0.318000       0          0      0   \n",
       "3        0.665000  0.260000      0.796400       0          0      0   \n",
       "4        0.776769  0.163923      0.200531       0          0      0   \n",
       "...           ...       ...           ...     ...        ...    ...   \n",
       "1681289  0.862000  0.115333      0.600333       0          0      0   \n",
       "1681290  0.648250  0.351750      0.367800       0          0      0   \n",
       "1681291  0.609000  0.000000     -0.670500       0          0      0   \n",
       "1681292  0.841500  0.096167     -0.002833       0          0      0   \n",
       "1681293  0.938833  0.040833      0.058200       0          0      0   \n",
       "\n",
       "         Public Description  Description  topic_type  \n",
       "0                         0            0           0  \n",
       "1                         0            0           0  \n",
       "2                         0            0           0  \n",
       "3                         0            0           0  \n",
       "4                         0            0           0  \n",
       "...                     ...          ...         ...  \n",
       "1681289                   0            0           0  \n",
       "1681290                   0            0           0  \n",
       "1681291                   0            0           0  \n",
       "1681292                   0            0           0  \n",
       "1681293                   0            0           0  \n",
       "\n",
       "[1681294 rows x 24 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Add all attributes to both node types, but values will be NaN for the non-node type attributes\n",
    "for col in sr_cols:\n",
    "    if col not in usr_df.columns:\n",
    "        usr_df[col] = 0\n",
    "usr_df['node_type'].replace('user', 0, inplace=True)\n",
    "usr_df['node_type'].replace('subreddit', 1, inplace=True)\n",
    "usr_df['is_sus'].replace(False, 0, inplace=True)\n",
    "usr_df['is_sus'].replace(True, 1, inplace=True)\n",
    "usr_df = usr_df.reindex(ordered_cols, axis=1)\n",
    "usr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting subreddit attributes...\n",
      "Setting user attributes...\n",
      "Setting default 0 values for nodes that didn't have attributes, marking node_type as subreddit...\n"
     ]
    }
   ],
   "source": [
    "# Re-apply node attributes if new node attributes defined\n",
    "name = \"mega\"\n",
    "#name = \"sus_usr_subreddits_posts\"\n",
    "\n",
    "out_dir = os.path.join(data_path, out_path, name)\n",
    "mega_net_df = pd.read_csv(os.path.join(out_dir, f\"{name}_bip.csv\"))\n",
    "# G = nx.from_pandas_edgelist(mega_net_df, 'node_x', 'node_y', edge_attr=['weight'])\n",
    "# G = add_node_attr(sr_df, usr_df, G)\n",
    "# nx.write_graphml(G, os.path.join(out_dir, f\"{name}_bip.graphml\"))\n",
    "mega_net_df_top100000 = mega_net_df.iloc[:100000,:]\n",
    "G_top100000 = nx.from_pandas_edgelist(mega_net_df_top100000, 'node_x', 'node_y', edge_attr=['weight'])\n",
    "G_top100000 = add_node_attr(sr_df, usr_df, G_top100000)\n",
    "nx.write_graphml(G_top100000, os.path.join(out_dir, f\"{name}_bip_top100000.graphml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_type</th>\n",
       "      <th>usr</th>\n",
       "      <th>top_sr</th>\n",
       "      <th>sr_set</th>\n",
       "      <th>num_sr</th>\n",
       "      <th>num_posts</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_popular_sr</th>\n",
       "      <th>num_political_sr</th>\n",
       "      <th>num_misc_sr</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_compound</th>\n",
       "      <th>is_sus</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Title</th>\n",
       "      <th>Public Description</th>\n",
       "      <th>Description</th>\n",
       "      <th>topic_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1532837</th>\n",
       "      <td>0</td>\n",
       "      <td>shomyo</td>\n",
       "      <td>uncen</td>\n",
       "      <td>{'linux', 'witcher', 'rule34', 'cringepics', '...</td>\n",
       "      <td>91</td>\n",
       "      <td>996</td>\n",
       "      <td>398</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188055</td>\n",
       "      <td>0.718294</td>\n",
       "      <td>0.092225</td>\n",
       "      <td>-0.206623</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         node_type     usr top_sr  \\\n",
       "1532837          0  shomyo  uncen   \n",
       "\n",
       "                                                    sr_set  num_sr  num_posts  \\\n",
       "1532837  {'linux', 'witcher', 'rule34', 'cringepics', '...      91        996   \n",
       "\n",
       "         num_comments  num_popular_sr  num_political_sr  num_misc_sr  ...  \\\n",
       "1532837           398              19                 1           66  ...   \n",
       "\n",
       "          avg_neg   avg_neu   avg_pos  avg_compound  is_sus  Subreddit  Title  \\\n",
       "1532837  0.188055  0.718294  0.092225     -0.206623       1        NaN    NaN   \n",
       "\n",
       "         Public Description  Description  topic_type  \n",
       "1532837                 NaN          NaN         NaN  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr_df[usr_df['usr'] == 'shomyo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make all from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parsing political_subreddits_new.csv (282927, 16) ===\n",
      "Original df size: (282927, 16)\n",
      "After removing messed up rows: (282927, 16)\n",
      "After removing known bots df size: (278127, 16)\n",
      "After drop dup author/link id and NaN df size: (269620, 12)\n",
      "=== Parsing popular_subreddits_new.csv (375710, 16) ===\n",
      "Original df size: (375710, 16)\n",
      "After removing messed up rows: (375710, 16)\n",
      "After removing known bots df size: (374114, 16)\n",
      "After drop dup author/link id and NaN df size: (366073, 12)\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\University\\networks_ml\\project\\m2\\m2_make_network.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m2/m2_make_network.ipynb#X16sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m name \u001b[39m=\u001b[39m fn\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m2/m2_make_network.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m type_sr \u001b[39m=\u001b[39m fn\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_subreddits\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m2/m2_make_network.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_path, fn))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m2/m2_make_network.ipynb#X16sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m=== Parsing \u001b[39m\u001b[39m{\u001b[39;00mfn\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mdf\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m ===\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m2/m2_make_network.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m df_clean \u001b[39m=\u001b[39m cleanup(df)\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "def make_networks(df_clean, name):\n",
    "        print(f\"~~~ Making {name} network ~~~~\")\n",
    "        out_dir = os.path.join(data_path, out_path, name)\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.mkdir(out_dir)\n",
    "        sr_df, usr_df, df = get_node_attr(df_clean)\n",
    "        # Bipartite\n",
    "        df = make_bipartite_graph(df_clean)\n",
    "        df.to_csv(os.path.join(out_dir, f\"{name}_bip.csv\"), index=False)\n",
    "        G = nx.from_pandas_edgelist(df, 'node_x', 'node_y', edge_attr=['weight'])\n",
    "        G = add_node_attr(sr_df, usr_df, G)\n",
    "        nx.write_graphml(G, os.path.join(out_dir, f\"{name}_bip.graphml\"))\n",
    "        if df.shape[0] > 100000:\n",
    "            df_top100000 = df.iloc[:100000,:]\n",
    "            G_top100000 = nx.from_pandas_edgelist(df_top100000, 'node_x', 'node_y', edge_attr=['weight'])\n",
    "            G_top100000 = add_node_attr(sr_df, usr_df, G_top100000)\n",
    "            nx.write_graphml(G_top100000, os.path.join(out_dir, f\"{name}_bip_top100000.graphml\"))\n",
    "\n",
    "# Separate per csv\n",
    "for fn in os.listdir(data_path):\n",
    "    if fn.endswith(\".csv\"):\n",
    "        name = fn.split('.')[0]\n",
    "        type_sr = fn.split('_subreddits')[0]\n",
    "        df = pd.read_csv(os.path.join(data_path, fn))\n",
    "        print(f\"=== Parsing {fn} {df.shape} ===\")\n",
    "        df_clean = cleanup(df)\n",
    "        # analyze(df_clean)\n",
    "        mega_df = pd.concat([mega_df, df_clean])\n",
    "        combo_category_df[type_sr] = pd.concat([combo_category_df[type_sr], df_clean])\n",
    "        if os.path.exists(os.path.join(data_path, out_path, name)):\n",
    "            continue\n",
    "        # df_clean = filter_min_commenters(df_clean)\n",
    "        make_networks(df_clean, name)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# Separate per category\n",
    "for sr,df in combo_category_df.items():\n",
    "    # df_clean = filter_min_commenters(df)\n",
    "    make_networks(df_clean, sr)\n",
    "    print(\"\\n\")\n",
    "     \n",
    "# Merge all\n",
    "mega_df = filter_min_commenters(mega_df)\n",
    "name = \"mega\"\n",
    "make_networks(mega_df, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parsing political_subreddits_new.csv (282927, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing popular_subreddits_new.csv (375710, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing sus_usr_subreddits_0_new.csv (687008, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing sus_usr_subreddits_10_new.csv (49622, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing sus_usr_subreddits_1_new.csv (667336, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing sus_usr_subreddits_2_new.csv (238142, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing sus_usr_subreddits_3_new.csv (995680, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing sus_usr_subreddits_4_new.csv (847003, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing sus_usr_subreddits_5_new.csv (237902, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing sus_usr_subreddits_6_new.csv (222456, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing sus_usr_subreddits_7_new.csv (469190, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing sus_usr_subreddits_8_new.csv (357967, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing sus_usr_subreddits_9_new.csv (668195, 16) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n",
      "=== Parsing sus_usr_subreddits_posts.csv (2299936, 17) ===\n",
      "Concat content..\n",
      "Obtaining polarity scores\n",
      "Creating new dataframe...\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data_scraping\\\\data_1month\" # Change this to path where .csv are located\n",
    "# Separate per csv\n",
    "for fn in os.listdir(data_path):\n",
    "    if fn.endswith(\".csv\"):\n",
    "        name = fn.split('.')[0]\n",
    "        df = pd.read_csv(os.path.join(data_path, fn))\n",
    "        print(f\"=== Parsing {fn} {df.shape} ===\")\n",
    "        df_sia = run_sentiment_analysis(df)\n",
    "        df_sia.to_csv(os.path.join(data_path, fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [2023-11-26 13:52:37.523143] Parsing political_subreddits_new.csv (282927, 20) ===\n",
      "Concat content..\n",
      "Obtaining toxicity scores on size (282927, 20)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\University\\networks_ml\\project\\m3\\scripts\\m3_make_network.ipynb Cell 48\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m3/scripts/m3_make_network.ipynb#Y102sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[:, \u001b[39m~\u001b[39mdf\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcontains(\u001b[39m'\u001b[39m\u001b[39m^Unnamed\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m3/scripts/m3_make_network.ipynb#Y102sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m=== [\u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m}\u001b[39;00m\u001b[39m] Parsing \u001b[39m\u001b[39m{\u001b[39;00mfn\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mdf\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m ===\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m3/scripts/m3_make_network.ipynb#Y102sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m df_toxic \u001b[39m=\u001b[39m run_toxic_analysis(df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m3/scripts/m3_make_network.ipynb#Y102sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m df_toxic\u001b[39m.\u001b[39mto_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, fn))\n",
      "\u001b[1;32md:\\Documents\\University\\networks_ml\\project\\m3\\scripts\\m3_make_network.ipynb Cell 48\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m3/scripts/m3_make_network.ipynb#Y102sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mselftext_title_body\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[cols]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(row\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m3/scripts/m3_make_network.ipynb#Y102sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mObtaining toxicity scores on size \u001b[39m\u001b[39m{\u001b[39;00mdf\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m3/scripts/m3_make_network.ipynb#Y102sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m res \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39mdf[\u001b[39m'\u001b[39;49m\u001b[39mselftext_title_body\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(pred\u001b[39m.\u001b[39;49mpredict)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m3/scripts/m3_make_network.ipynb#Y102sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCreating new dataframe...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/University/networks_ml/project/m3/scripts/m3_make_network.ipynb#Y102sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m sentiment_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_records(res)\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\pandas\\core\\series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4751\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4752\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4753\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4754\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   4755\u001b[0m         func,\n\u001b[0;32m   4756\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[0;32m   4757\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[0;32m   4758\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   4759\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m-> 4760\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\pandas\\core\\apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[0;32m   1206\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1207\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\pandas\\core\\apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[0;32m   1288\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[0;32m   1289\u001b[0m )\n\u001b[0;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1292\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mD:\\Documents\\University\\networks_ml\\project\\m3\\detoxify\\detoxify\\detoxify.py:116\u001b[0m, in \u001b[0;36mDetoxify.predict\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m    115\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 116\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    117\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(out)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    118\u001b[0m results \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:1226\u001b[0m, in \u001b[0;36mXLMRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1226\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1227\u001b[0m     input_ids,\n\u001b[0;32m   1228\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1229\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1230\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1231\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1232\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1233\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1234\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1235\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1236\u001b[0m )\n\u001b[0;32m   1237\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1238\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:854\u001b[0m, in \u001b[0;36mXLMRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    845\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    847\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    848\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    849\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    852\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    853\u001b[0m )\n\u001b[1;32m--> 854\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    855\u001b[0m     embedding_output,\n\u001b[0;32m    856\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    857\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    858\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    859\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    860\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    861\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    862\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    863\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    864\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    865\u001b[0m )\n\u001b[0;32m    866\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    867\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:528\u001b[0m, in \u001b[0;36mXLMRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    519\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    520\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    521\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    525\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    526\u001b[0m     )\n\u001b[0;32m    527\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 528\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    529\u001b[0m         hidden_states,\n\u001b[0;32m    530\u001b[0m         attention_mask,\n\u001b[0;32m    531\u001b[0m         layer_head_mask,\n\u001b[0;32m    532\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    533\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    534\u001b[0m         past_key_value,\n\u001b[0;32m    535\u001b[0m         output_attentions,\n\u001b[0;32m    536\u001b[0m     )\n\u001b[0;32m    538\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    539\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:454\u001b[0m, in \u001b[0;36mXLMRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    451\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    452\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 454\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    455\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    456\u001b[0m )\n\u001b[0;32m    457\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    459\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:466\u001b[0m, in \u001b[0;36mXLMRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m--> 466\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[0;32m    467\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    468\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:364\u001b[0m, in \u001b[0;36mXLMRobertaIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 364\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    365\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Melissa\\miniconda3\\envs\\network-ml\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_path = \"data_scraping\\\\data_1month\" # Change this to path where .csv are located\n",
    "# Separate per csv\n",
    "for fn in os.listdir(data_path):\n",
    "    if fn.endswith(\".csv\"):\n",
    "        name = fn.split('.')[0]\n",
    "        df = pd.read_csv(os.path.join(data_path, fn), index_col=0)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        print(f\"=== [{datetime.now()}] Parsing {fn} {df.shape} ===\")\n",
    "        df_toxic = run_toxic_analysis(df)\n",
    "        df_toxic.to_csv(os.path.join(data_path, fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [2023-11-26 17:41:30.822883] Parsing political_subreddits_new.csv (282927, 20) ===\n",
      "Calculating similarity on size (103488, 2)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data_scraping\\\\data_1month\" # Change this to path where .csv are located\n",
    "out_path = os.path.join(data_path, 'usr_comment_similarity.csv')\n",
    "# Separate per csv\n",
    "for fn in os.listdir(data_path):\n",
    "    if fn.endswith(\".csv\"):\n",
    "        name = fn.split('.')[0]\n",
    "        df = pd.read_csv(os.path.join(data_path, fn), index_col=0)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        print(f\"=== [{datetime.now()}] Parsing {fn} {df.shape} ===\")\n",
    "        df = find_comment_similarity(df)\n",
    "        if os.path.isfile(out_path):\n",
    "            df.to_csv(out_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            df.to_csv(out_path, header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
